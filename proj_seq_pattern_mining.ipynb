{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poject: Sequential Pattern Mining with Web log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we will use spark and the prefix algorithm to mine a web log from an online store.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: <font color=\"blue\">Mingjia \"Jacky\" Guan</font>\n",
    "\n",
    "E-mail: <font color=\"blue\">mguan@stu.feitian.edu</font>\n",
    "\n",
    "Date: <font color=\"blue\">04/05/2024</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Data and Cleaning\n",
    "\n",
    "**Dataset**: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/3QBYB5\n",
    "\n",
    "- Alternative download link:  https://cis331.guihang.org/data/access.log.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Download and unzip** the file `Access.log.zip` ; Move the file `access.log` to the directory that you map into docker instance:\n",
    "\n",
    "Say, I move my file into my home: `/Users/zhengqu` ; then in Mac terminal/Windows command prompt, I \"cd\" to my home dir (just type \"cd\" and enter, it will do).\n",
    "\n",
    "```bash\n",
    "pwd\n",
    "```\n",
    "\n",
    "<font size=-1 color=grey>/Users/zhengqu</font>\n",
    "\n",
    "Let's assume I put `access.log` in my home dir as `/Users/zhengqu/access.log` and my spark docker container/instance was created by the following command, as I demonstrated in the video after the spark lab lecture:\n",
    "\n",
    "```bash\n",
    "docker run -p 10000:8888 -d -P --name notebook -it  -v \"$PWD:/home/jovyan/mydir\" jupyter/all-spark-notebook\n",
    "```\n",
    "\n",
    "whereby `\"$PWD\"` is actually `/Users/zhengqu` in my case, and it is mapped into docker container `notebook` as `/home/jovyan/mydir`. So anything I put in this folder `\"/Users/zhengqu\"` on my host laptop is available in my spark docker container `notebook` in this folder: `/home/jovyan/mydir`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the data file from docker container\n",
    "\n",
    "- First you start the spark docker contaner/instance (you don't have to if your docker container is not stopped since last running) \n",
    "\n",
    "Use this command to show docker container running status:\n",
    "\n",
    "```bash\n",
    "docker ps -a\n",
    "```\n",
    "\n",
    "- To start /start container:\n",
    "\n",
    "```bash\n",
    "docker start notebook\n",
    "```\n",
    "\n",
    "- Next you connect your container with bash:\n",
    "\n",
    "```bash\n",
    "# connect to docker container with bash\n",
    "docker exec -it notebook /bin/bash\n",
    "```\n",
    "\n",
    "- Now you are inside the spark docker container (imagine it as a virtual linux machine).\n",
    "\n",
    "----\n",
    "**Note: commands below are run from docker container linux system**\n",
    "\n",
    "\n",
    "- Let's \"cd\" to the mapped folder:\n",
    "\n",
    "```bash\n",
    "# change to mapped dir:\n",
    "cd /home/jovyan/mydir\n",
    "```\n",
    "\n",
    "\n",
    "- Let's check the file by running: \n",
    "\n",
    "```bash\n",
    " ls -l access.log\n",
    "```\n",
    "<font size=-2 color=grey>`-rw-r--r--@ 1 jovyan  staff 3502440823 Jan 26  2019 access.log`</font>\n",
    "\n",
    "- Let's check how many lines it has by running (this takes a while):\n",
    "\n",
    "```bash\n",
    "wc -l access.log\n",
    "```\n",
    "<font size=-2 color=grey>`10365152 access.log`</font>\n",
    "\n",
    "- Let's print the last 100 lines from end of the file:\n",
    "\n",
    "```bash\n",
    "tail -n 100 access.log\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font size=-2 color=#524d43>\n",
    "\n",
    "```\n",
    "151.239.2.90 - - [26/Jan/2019:20:29:09 +0330] \"GET /static/bundle-bundle_site_head.css HTTP/1.1\" 499 0 \"https://www.zanbil.ir/search/%D8%B3%D8%B1%D8%B1%D8%B3%DB%8C%D8%AF/p0\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:64.0) Gecko/20100101 Firefox/64.0\" \"-\"\n",
    "151.239.2.90 - - [26/Jan/2019:20:29:09 +0330] \"GET /image/%7B%7BbasketItem.id%7D%7D?type=productModel&wh=50x50 HTTP/1.1\" 200 5 \"https://www.zanbil.ir/search/%D8%B3%D8%B1%D8%B1%D8%B3%DB%8C%D8%AF/p0\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:64.0) Gecko/20100101 Firefox/64.0\" \"-\"\n",
    "5.114.175.169 - - [26/Jan/2019:20:29:09 +0330] \"GET /image/56602/productModel/200x200 HTTP/1.1\" 200 6188 \"https://www.zanbil.ir/m/filter/b2%2Cp65%2Ct31\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 11_4_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.0 Mobile/15E148 Safari/604.1\" \"-\"\n",
    "5.114.175.169 - - [26/Jan/2019:20:29:09 +0330] \"GET /image/62175/productModel/200x200 HTTP/1.1\" 200 8140 \"https://www.zanbil.ir/m/filter/b2%2Cp65%2Ct31\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 11_4_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.0 Mobile/15E148 Safari/604.1\" \"-\"\n",
    "5.114.175.169 - - [26/Jan/2019:20:29:09 +0330] \"GET /image/62064/productModel/200x200 HTTP/1.1\" 200 6840 \"https://www.zanbil.ir/m/filter/b2%2Cp65%2Ct31\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 11_4_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.0 Mobile/15E148 Safari/604.1\" \"-\"\n",
    "151.239.2.90 - - [26/Jan/2019:20:29:09 +0330] \"GET /static/images/search-category-arrow.png HTTP/1.1\" 200 217 \"https://znbl.ir/static/bundle-bundle_site_head.css\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:64.0) Gecko/20100101 Firefox/64.0\" \"-\"\n",
    "[truncated]\n",
    "```\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Understand the Web Log Format\n",
    "\n",
    "You want to read [this article](https://www.sumologic.com/blog/apache-access-log/) to understand the web log format. Each web site may have a slightly different format. In general, you will see a user IP at the beginning, then the timestamp (in `[ ]` in above case) and user request (in simplified terms, sending data is called `POST`, like send login password; retrieving data is called `GET`). The request URL doesn't contain the base domain name. So \n",
    "\n",
    "```\n",
    "GET /image/62175/productModel/200x200\n",
    "```\n",
    "\n",
    "means retrieving https://www.zanbil.ir/mage/62175/productModel/200x200\n",
    "(the base domain name of this web log is www.zanbil.ir)\n",
    "\n",
    "The 3 digits number after `HTTP/1.1` is the access status: 200 means success, 3** means success but user previously requested the same data so it might be served from browser cache (saved data in user browser), 4** means client (user browser) error (404 is the infamous non-existing status); 5** means server error. <font color=\"orange\">So you only care about 200 and 3** status, and want to filter out lines with other status types.</font>\n",
    "\n",
    "You will see each IP at one timestamp requesting data from many URLs simultaneously--that's because when you click a page, multiple images, css, javascript files within that page will be retrieved and each leaves a separate web log entry. \n",
    "\n",
    "\n",
    "<font color=\"orange\">You want to remember before mining further you will filter out all lines containing requesting css or js (you want to manually figure out patterns for these types of urls) which are normally useless for our purpose.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also notice some log entry contain this type of lines:\n",
    "\n",
    "<font size=-2 color=#888>\n",
    "```\n",
    "37.129.59.160 - - [26/Jan/2019:20:29:12 +0330] \"GET /basket/add/62424?mobile=1&addedValues= HTTP/1.1\" 302 0 \"https://www-zanbil-ir.cdn.ampproject.org/v/s/www.zanbil.ir/m/product/32148/%DA%AF%D9%88%D8%B4%DB%8C-%D8%AA%D9%84%D9%81%D9%86-%D8%A8%DB%8C-%D8%B3%DB%8C%D9%85-%D9%BE%D8%A7%D9%86%D8%A7%D8%B3%D9%88%D9%86%DB%8C%DA%A9-%D9%85%D8%AF%D9%84-Panasonic-Cordless-Telephone-KX-TGC412?amp_js_v=0.1&usqp=mq331AQECAEoAQ%3D%3D\" \"Mozilla/5.0 (Linux; Android 6.0.1; D6633 Build/23.5.A.1.291) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Mobile Safari/537.36\" \"-\"\n",
    "```\n",
    "</font>\n",
    "\n",
    "- Note: <font color=\"orange\">this URL `/basket/add/62424` means the user's action to add a product to the basket!</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Preprocess Tasks \n",
    "\n",
    "To simplify our task, we will only consider users who have added at least a product to the basket. So preprocess steps involve:\n",
    "\n",
    "0. read the access.log into spark`*`\n",
    "1. filter out all unnecessary lines as highlighted in orange before.\n",
    "2. Get a list of IPs (let's call it uniqueIPs) who performed the action of adding product(s) to the basket \n",
    "3. filter out all records whose IP does not belong to the IP list in step 2.\n",
    "\n",
    "The remaining records are what we will mine with prefixspan.\n",
    "\n",
    "`*` For step 0, for reading text file into spark, follow examples on this page (click python version): https://spark.apache.org/docs/latest/mllib-frequent-pattern-mining.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example on a notebook running from docker container\n",
    "\n",
    "- Run this from docker container's Linux bash command line to show jupyter notebook tokens\n",
    "\n",
    "```\n",
    "jupyter notebook list\n",
    "```\n",
    "\n",
    "<font size=-2 color=grey>\n",
    "\n",
    "```\n",
    "Currently running servers:\n",
    "http://0.0.0.0:8888/?token=01e38a2234531e653d320df569dbc94173ebe89460cd57a :: /home/jovyan\n",
    "```\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your laptop browser you open this url:\n",
    "`http://127.0.0.1:10000/?token=01e38a2234531e653d320df569dbc94173ebe89460cd57a`\n",
    "\n",
    "Note you replace 8888 with 10000 , then you navigate to your mapped dir which is `mydir` if you follow above commands. Then you do \"file\" and new notebook, save the new notebook (file>save notebook). You will also see the new notebook in your laptop folder, in my case it's in `/Users/zhengqu` -- this notebook is running in docker container, so do not simultaneously open the same notebook in host laptop (Windows/MAC)'s jupyter! This creates conflict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see pyspark example from https://spark.apache.org/docs/latest/mllib-frequent-pattern-mining.html\n",
    "\n",
    "\n",
    "```python\n",
    "from pyspark.mllib.fpm import FPGrowth\n",
    "from pyspark import SparkContext \n",
    "sc = SparkContext('local[*]')\n",
    "\n",
    "# Note data is an object of RDD: pyspark.rdd.PipelinedRDD\n",
    "data = sc.textFile(\"data/mllib/sample_fpgrowth.txt\") \n",
    "transactions = data.map(lambda line: line.strip().split(' '))\n",
    "model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
    "result = model.freqItemsets().collect() ## << note .collect() \n",
    "for fi in result:\n",
    "    print(fi)\n",
    "```\n",
    "\n",
    "Please run this example. Note \n",
    "- you need to download `sample_fpgrowth.txt` from \n",
    "https://github.com/apache/spark/raw/master/data/mllib/sample_fpgrowth.txt\n",
    "and properly specify the file path in `sc.textFile()`. You can use `!wget URL-to-data` in jupyter notebook, then `!ls path/to/downloaded/file` to verify the file\n",
    "- The spark object collection (data frame or pipelines) cannot be iterated until be applied with `.collect()` as in above example or using  `.take(n)` as in demo in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>You code to run example of sample_fpgrowth.txt with FPGrowth here </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-05 23:37:09--  https://github.com/apache/spark/raw/master/data/mllib/sample_fpgrowth.txt\n",
      "Resolving github.com (github.com)... 140.82.113.3\n",
      "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_fpgrowth.txt [following]\n",
      "--2024-04-05 23:37:09--  https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_fpgrowth.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 68 [text/plain]\n",
      "Saving to: ‘sample_fpgrowth.txt’\n",
      "\n",
      "sample_fpgrowth.txt 100%[===================>]      68  --.-KB/s    in 0s      \n",
      "\n",
      "2024-04-05 23:37:09 (5.90 MB/s) - ‘sample_fpgrowth.txt’ saved [68/68]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://github.com/apache/spark/raw/master/data/mllib/sample_fpgrowth.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FreqItemset(items=['z'], freq=5)\n",
      "FreqItemset(items=['x'], freq=4)\n",
      "FreqItemset(items=['x', 'z'], freq=3)\n",
      "FreqItemset(items=['y'], freq=3)\n",
      "FreqItemset(items=['y', 'x'], freq=3)\n",
      "FreqItemset(items=['y', 'x', 'z'], freq=3)\n",
      "FreqItemset(items=['y', 'z'], freq=3)\n",
      "FreqItemset(items=['r'], freq=3)\n",
      "FreqItemset(items=['r', 'x'], freq=2)\n",
      "FreqItemset(items=['r', 'z'], freq=2)\n",
      "FreqItemset(items=['s'], freq=3)\n",
      "FreqItemset(items=['s', 'y'], freq=2)\n",
      "FreqItemset(items=['s', 'y', 'x'], freq=2)\n",
      "FreqItemset(items=['s', 'y', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['s', 'y', 'z'], freq=2)\n",
      "FreqItemset(items=['s', 'x'], freq=3)\n",
      "FreqItemset(items=['s', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['s', 'z'], freq=2)\n",
      "FreqItemset(items=['t'], freq=3)\n",
      "FreqItemset(items=['t', 'y'], freq=3)\n",
      "FreqItemset(items=['t', 'y', 'x'], freq=3)\n",
      "FreqItemset(items=['t', 'y', 'x', 'z'], freq=3)\n",
      "FreqItemset(items=['t', 'y', 'z'], freq=3)\n",
      "FreqItemset(items=['t', 's'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'y'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'y', 'x'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'y', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'y', 'z'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'x'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'z'], freq=2)\n",
      "FreqItemset(items=['t', 'x'], freq=3)\n",
      "FreqItemset(items=['t', 'x', 'z'], freq=3)\n",
      "FreqItemset(items=['t', 'z'], freq=3)\n",
      "FreqItemset(items=['p'], freq=2)\n",
      "FreqItemset(items=['p', 'r'], freq=2)\n",
      "FreqItemset(items=['p', 'r', 'z'], freq=2)\n",
      "FreqItemset(items=['p', 'z'], freq=2)\n",
      "FreqItemset(items=['q'], freq=2)\n",
      "FreqItemset(items=['q', 'y'], freq=2)\n",
      "FreqItemset(items=['q', 'y', 'x'], freq=2)\n",
      "FreqItemset(items=['q', 'y', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 'y', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 't'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'y'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'y', 'x'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'y', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'y', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'x'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 'x'], freq=2)\n",
      "FreqItemset(items=['q', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 'z'], freq=2)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.fpm import FPGrowth\n",
    "\n",
    "data = sc.textFile(\"sample_fpgrowth.txt\")\n",
    "transactions = data.map(lambda line: line.strip().split(' '))\n",
    "model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n",
    "for fi in result:\n",
    "    print(fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "- data is an object of RDD: \n",
    "```python\n",
    "type(data)\n",
    "```\n",
    "> <font size=-2>pyspark.rdd.PipelinedRDD</font>\n",
    "- `sc.textFile()` takes the path to your input file. Use jupyter command `!ls path/to/access.log` to verify your file path. Or switch to docker container command line, using something like `ls /home/jovyan/mydir/access.log` to verify.\n",
    "- `data.map()` takes a function to be applied to each line in the input text file. If you prefer not using lambda function, you can define a named function like:\n",
    "```python\n",
    "def myParser (line): \n",
    "   return line.strip().split(' ')\n",
    "```\n",
    "\n",
    "And then you can use `data.map` like:\n",
    "```python\n",
    "transactions = data.map(myParser)\n",
    "```\n",
    "\n",
    "Please try it in the example. Also reference notebook \"pyspark-ex.ipynb\" used in my video demo for docker/spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/mydir/workspaces/CIS331/sequential_pattern_mining\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>You code to run example of sample_fpgrowth.txt with myParser here </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FreqItemset(items=['z'], freq=5)\n",
      "FreqItemset(items=['x'], freq=4)\n",
      "FreqItemset(items=['x', 'z'], freq=3)\n",
      "FreqItemset(items=['y'], freq=3)\n",
      "FreqItemset(items=['y', 'x'], freq=3)\n",
      "FreqItemset(items=['y', 'x', 'z'], freq=3)\n",
      "FreqItemset(items=['y', 'z'], freq=3)\n",
      "FreqItemset(items=['r'], freq=3)\n",
      "FreqItemset(items=['r', 'x'], freq=2)\n",
      "FreqItemset(items=['r', 'z'], freq=2)\n",
      "FreqItemset(items=['s'], freq=3)\n",
      "FreqItemset(items=['s', 'y'], freq=2)\n",
      "FreqItemset(items=['s', 'y', 'x'], freq=2)\n",
      "FreqItemset(items=['s', 'y', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['s', 'y', 'z'], freq=2)\n",
      "FreqItemset(items=['s', 'x'], freq=3)\n",
      "FreqItemset(items=['s', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['s', 'z'], freq=2)\n",
      "FreqItemset(items=['t'], freq=3)\n",
      "FreqItemset(items=['t', 'y'], freq=3)\n",
      "FreqItemset(items=['t', 'y', 'x'], freq=3)\n",
      "FreqItemset(items=['t', 'y', 'x', 'z'], freq=3)\n",
      "FreqItemset(items=['t', 'y', 'z'], freq=3)\n",
      "FreqItemset(items=['t', 's'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'y'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'y', 'x'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'y', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'y', 'z'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'x'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['t', 's', 'z'], freq=2)\n",
      "FreqItemset(items=['t', 'x'], freq=3)\n",
      "FreqItemset(items=['t', 'x', 'z'], freq=3)\n",
      "FreqItemset(items=['t', 'z'], freq=3)\n",
      "FreqItemset(items=['p'], freq=2)\n",
      "FreqItemset(items=['p', 'r'], freq=2)\n",
      "FreqItemset(items=['p', 'r', 'z'], freq=2)\n",
      "FreqItemset(items=['p', 'z'], freq=2)\n",
      "FreqItemset(items=['q'], freq=2)\n",
      "FreqItemset(items=['q', 'y'], freq=2)\n",
      "FreqItemset(items=['q', 'y', 'x'], freq=2)\n",
      "FreqItemset(items=['q', 'y', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 'y', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 't'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'y'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'y', 'x'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'y', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'y', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'x'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 't', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 'x'], freq=2)\n",
      "FreqItemset(items=['q', 'x', 'z'], freq=2)\n",
      "FreqItemset(items=['q', 'z'], freq=2)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.fpm import FPGrowth\n",
    "\n",
    "def myParser(line):\n",
    "    return line.strip().split(\" \")\n",
    "\n",
    "data = sc.textFile(\"sample_fpgrowth.txt\")\n",
    "transactions = data.map(myParser)\n",
    "model = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\n",
    "result = model.freqItemsets().collect()\n",
    "for fi in result:\n",
    "    print(fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and parse data in spark\n",
    "\n",
    "In this project, your function, let's still call it `myParser` for `data.map()` should take a line of string and  return a tuple of\n",
    "\n",
    "`IP, TIMESTAMP, REQUEST_URL, STATUS`\n",
    "\n",
    "- `IP` can be a string, \n",
    "- `TIMESTAMP` should be a datetime object, refer to [this tutorial](https://stackabuse.com/converting-strings-to-datetime-in-python/) for converting string to datetime object. (note: timezone doesn't matter in this project because all records are in the same timezone, which is server time, so you can ignore it.)\n",
    "- `REQUEST_URL` is a string\n",
    "- STATUS is integer, 200, 302 etc.\n",
    "\n",
    "\n",
    "It's a good habit to test your function `myParser` first, with a smaller dataset and without spark, before proceeding. You can create a 20 line file for this by running this in docker container Linux command line:\n",
    "```bash\n",
    "tail -n 20 access.log > small20.log\n",
    "```\n",
    "\n",
    "This will create a file \"small20.log\"\n",
    "\n",
    "You can view it by running following command in docker container Linux command:\n",
    "```bash\n",
    "cat small20.log\n",
    "```\n",
    "\n",
    "Now you can test this file (note it's sitting inside `\"/home/jovyan/mydir\"` ) by following python code in Jupyter notebook:\n",
    "\n",
    "```python\n",
    "# Note this test doesn't involve spark\n",
    "lines = open(\"/home/jovyan/mydir/small20.log\").readlines()\n",
    "for l in map(myParser, lines):\n",
    "   print(l)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>You code for defining function `myParser` and testing it on small20.log without using spark (you can use code above) here </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After your test, apply your function `myParser` in spark RDD and print out 15 rows （use data.take(15) ）from your large spark data (based upon the large access.log file), note each row will be:\n",
    "\n",
    "IP, TIMESTAMP, REQUEST_URL, STATUS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>You code to test `myParser` on reading access.log as spark RDD and print out 15 rows here </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the spark data\n",
    "\n",
    "Now you want to filter your RDD. We completed step 0 below. The remaining steps are 1,2,3.\n",
    "\n",
    "0. read the access.log into spark (**completed**)\n",
    "1. filter out all unnecessary lines as highlighted before.\n",
    "2. Get a list of IPs (let's call it uniqueIPs) who performed the action of adding product(s) to the basket \n",
    "3. filter out all records whose IP does not belong to the IP list in step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting RDD to dataframe\n",
    "\n",
    "In order to do filtering operations, we want to convert the RDD into spark dataframe. Let's run and study following examples. \n",
    "\n",
    "**Note**: in case you get error with `ValueError: Cannot run multiple SparkContexts at once` you need to stop previous `SparkContext` by running\n",
    "```python\n",
    "sc.stop()\n",
    "```\n",
    "\n",
    "Or you comment the line for `sc = SparkContext('local[*]')`\n",
    "\n",
    "**Important** Run and **study** the code example below carefully, you might use every line of it, with modification, for the project.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "sc = SparkContext('local[*]') # comment this in case you already have SparkContext running\n",
    "\n",
    "\n",
    "\n",
    "dataList = [(\"James\", \"Cameron\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\", \"Johnson\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\", \"Stromberg\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\", \"McDonald\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Pearson\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Kindel\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Lewis\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\", \"Scott\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\", \"Jobs\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "rdd=sc.parallelize(dataList) # Note RDD is created from list, similar to creating it from a text file\n",
    "\n",
    "# *** Now we convert rdd to dataframe\n",
    "spark = SparkSession(sc) # this is necessary for rdd to be converted to data frame\n",
    "\n",
    "# converting RDD to DataFrame\n",
    "df = rdd.toDF(schema = [\"first_name\", \"last_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"])\n",
    "\n",
    "# Show schema\n",
    "df.printSchema()\n",
    "\n",
    "# Show data frame\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Note we create two new columns from grouped data:\n",
    "# they are renamed as dep_salary, dep_individual_salary\n",
    "df.groupBy(\"department\").agg(F.sum('salary').alias(\"dep_salary\"), \n",
    "          F.collect_list('salary').alias(\"dep_individual_salary\")).show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual steps for filtering the spark data\n",
    "\n",
    "Now you write your code to \n",
    "\n",
    "1. Convert spark RDD to Data Frame, then filter out all unnecessary lines as highlighted in <font color=orange>orange</font> in the above section `Understand the Web Log Format`.\n",
    "2. Get a list of IPs (let's call it `uniqueIPs`) who performed the action of adding product(s) to the basket \n",
    "3. filter out all records whose IP does not belong to the IP list in step 2.\n",
    "4. Note step 2 and 3 should not be performed literally. I am using the language for easy understanding. This will be explained further in next section (see `last note`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important resources for wrangling spark data, should you encounter problems\n",
    "\n",
    "- When testing your code you have to go through many trial and error cycles; It may speed up your coding process by creating a reletively smaller data file (say, 10,000 ~ 30,000 lines) to test your code until it succeeds.\n",
    "- Please bookmark [this tutorial](https://sparkbyexamples.com/) and study relevant examples (browse the menu on the left) if you have trouble with certain functions.\n",
    "- You may want to explore `where` or `filter`, `groupBy`, `agg`, `withColumn`, `Date and Timestamp Functions`, `udf` (user defined function)\n",
    "- **Note** the examples in general ommited import statements. If you have errors saying certain objects not defined, it may be resolved by \n",
    "```python\n",
    "from pyspark.sql.functions import *\n",
    "from from pyspark.sql.types import *\n",
    "```\n",
    "Or you search google with something like `pyspark import udf`\n",
    "- One useful function to be applied to data frame column is `.isin()` which takes a list and check column elements against that list. see https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.isin.html#pyspark.sql.Column.isin\n",
    "- To convert a spark dataframe column into a python list, you need to use toPandas then pandas function will work, for example to get first name list from above example df:\n",
    "```python\n",
    "h = df.select(\"first_name\").toPandas()\n",
    "huge_list = list( h[\"first_name\"].values)\n",
    "huge_list\n",
    "```\n",
    "\n",
    "- **Last note**: Filtering a spark data frame with a list could be extremely slow if you deal with huge list and big data frame. Standard approach is to use two dataframes and apply [inner join](https://sparkbyexamples.com/pyspark/pyspark-join-explained-with-examples/) for this scenario. So please use inner join or simply use sql syntax. If you stick with `.isin(huge_list)` you get 2 points deduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>You code for filtering spark data here </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Sequential Data for prefixspan \n",
    "\n",
    "After you get all data in spark and filter out unnecessary records, we want to prepare the sequential data for prefixspan.\n",
    "\n",
    "In this project we define a sequence of events as:\n",
    " \n",
    "> A series of URLs requested by one particular user (a user is identified by the IP address), within one day.\n",
    "e.g., on 26/Jan/2019 a user visited urls at  `/A` then `/B` then at the same timestamp `/C` and `/D`, this will form a sequence as a list of list:\n",
    "\n",
    "```python\n",
    "[ [\"/A\"], [\"/B\"], [\"/C\", \"/D\"] ]\n",
    "```\n",
    "\n",
    "So now with your spark data frame (assuming all irrelevant rows filtered out), let's still call it `df`, you want to:\n",
    "\n",
    "\n",
    "1. Group the dataframe by `(IP, TIMESTAMP)` so actions from same IP and at the same timestamp are grouped together\n",
    "  - now each group contains one or a set of urls which we define as one event of the sequence\n",
    "  - you want to apply an aggregation (`agg`) function to create a **list of URLs** for each group, let's call this new aggregated column `EVENTS` and let's call this aggregated data frame `DF2` (refer to reources listed above and refer to the example code above in `Converting RDD to dataframe` section)\n",
    "2. From `DF2` create a new column called `DATE` which converts the `TIMESTAMP` into a date object so timestamps of the same day have the same values in this column (refer to reources listed above).\n",
    "3. Since `DATE` column round timestamp to date so if you group the data frame by `(IP, DATE)` you will have in  each group a list of `EVENTS` from each user in each day  -- this is exactly the sequence we defined. So after grouping, just create another aggregated column `SEQUENCE` which consists list of `EVENTS` of the group (refer to the example code above in `Converting RDD to dataframe` section)\n",
    "4. Finally you want to print out 30 rows from column `SEQUENCE` to verify you did it right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>You code for `Prepare Sequential Data for prefixspan` here </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show time\n",
    "\n",
    "Now you want to use spark library for prefixspan to mine the sequence data. This is just a simple call:\n",
    "https://spark.apache.org/docs/latest/api/python//reference/api/pyspark.ml.fpm.PrefixSpan.html\n",
    "\n",
    "- Note default sequence column in the input data frame is `sequence`, you can change that to match your data frame:\n",
    "```python\n",
    "prefixSpan.setSequenceCol('SEQUENCE')\n",
    "```\n",
    "- you want to play with `MinSupport` and `MaxPatternLength` in order to get meaningful patterns. Start with higher `MinSupport` value (0.5 is a really high value but you can start with it) and lower `MaxPatternLength` (5 is a good start) to speed up your test run.\n",
    "\n",
    "- You want to write a summary about your observed patterns in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>You code for doing prefixspan here </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>You summary and clusions here </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern evaluation: at this stage you should try your best to filter out unrelevant \"rules\" and present only as much as possible valuable information for your mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DELIVER (individually)\n",
    "\n",
    "\n",
    "\n",
    "Submit on canvas:\n",
    "\n",
    "* This notebook with all results shown\n",
    "* If you discussed with classmates and get inspirations during the project write down his/her names in next section. No points deductions but you have to be honest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\" color=\"#003300\">I hereby declare that, except for the code provided by the course instructors, all of my code, report, and figures were produced by myself.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
